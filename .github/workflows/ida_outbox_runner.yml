# tools/outbox_runner.py
from __future__ import annotations

import os
import json
import glob
import shutil
from datetime import datetime, timezone
from typing import Any, Dict, Optional, Tuple

from tools.job_schema import load_job_from_file, JobSchemaError, Job


OUTBOX_DIR = os.environ.get("OUTBOX_DIR", "agent_outbox")
RESULTS_DIR = os.environ.get("RESULTS_DIR", "agent_results")
OPS_LOG = os.environ.get("OPS_LOG", "ops/logs/runner.log")

# Hard safety knobs (prod-friendly defaults)
MAX_JOBS_PER_RUN = int(os.environ.get("MAX_JOBS_PER_RUN", "1"))  # 1 per tick = stabil drift
BUDGET_SECONDS = int(os.environ.get("BUDGET_SECONDS", "110"))     # keep below GH action step budget


def utc_now() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")


def ensure_dirs() -> None:
    os.makedirs(OUTBOX_DIR, exist_ok=True)
    os.makedirs(os.path.join(OUTBOX_DIR, "done"), exist_ok=True)
    os.makedirs(os.path.join(OUTBOX_DIR, "failed"), exist_ok=True)
    os.makedirs(RESULTS_DIR, exist_ok=True)
    os.makedirs(os.path.dirname(OPS_LOG), exist_ok=True)


def log_line(msg: str) -> None:
    line = f"[{utc_now()}] {msg}"
    print(line, flush=True)
    with open(OPS_LOG, "a", encoding="utf-8") as f:
        f.write(line + "\n")


def safe_write_json(path: str, data: Dict[str, Any]) -> None:
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, sort_keys=True)
    os.replace(tmp, path)


def move_job(src_path: str, dest_folder: str) -> str:
    os.makedirs(dest_folder, exist_ok=True)
    base = os.path.basename(src_path)
    dest = os.path.join(dest_folder, base)
    # Overwrite-safe: if exists, add suffix
    if os.path.exists(dest):
        stem, ext = os.path.splitext(base)
        dest = os.path.join(dest_folder, f"{stem}-{int(datetime.now().timestamp())}{ext}")
    shutil.move(src_path, dest)
    return dest


def pick_fifo_job() -> Optional[str]:
    # FIFO by filename (your generator uses timestamps => works)
    paths = sorted(glob.glob(os.path.join(OUTBOX_DIR, "*.json")))
    return paths[0] if paths else None


def make_result(job: Optional[Job], status: str, detail: str, extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    payload: Dict[str, Any] = {
        "ts_utc": utc_now(),
        "status": status,           # ok | error | skipped
        "detail": detail,
    }
    if job is not None:
        payload["job_id"] = job.id
        payload["job_type"] = job.type
    if extra:
        payload.update(extra)
    return payload


def execute_job(job: Job) -> Tuple[str, Dict[str, Any]]:
    """
    A2: We DO NOT implement tools yet (thatâ€™s A3/B).
    We only acknowledge the job, and record what would run.
    """
    return ("ok", {"message": "validated_and_queued_for_execution_layer", "payload_keys": sorted(list(job.payload.keys()))})


def main() -> None:
    ensure_dirs()

    start = datetime.now(timezone.utc)
    processed = 0

    # Diagnostics
    pending = sorted(glob.glob(os.path.join(OUTBOX_DIR, "*.json")))
    log_line(f"Start run. budget={BUDGET_SECONDS}s, jobs_in_outbox={len(pending)}, max_per_run={MAX_JOBS_PER_RUN}")

    while processed < MAX_JOBS_PER_RUN:
        # budget guard
        elapsed = (datetime.now(timezone.utc) - start).total_seconds()
        if elapsed >= BUDGET_SECONDS:
            log_line(f"Budget reached. elapsed={elapsed:.1f}s, processed={processed}")
            break

        job_path = pick_fifo_job()
        if not job_path:
            log_line("No jobs in outbox. Exiting.")
            break

        log_line(f"Picked job FIFO: {os.path.basename(job_path)}")

        # Parse + validate
        try:
            job = load_job_from_file(job_path)
        except (json.JSONDecodeError, JobSchemaError) as e:
            # Write error result + move job to failed
            err_res = make_result(None, "error", f"invalid_job: {str(e)}", {"file": os.path.basename(job_path)})
            out_name = os.path.splitext(os.path.basename(job_path))[0] + ".error.json"
            safe_write_json(os.path.join(RESULTS_DIR, out_name), err_res)
            moved = move_job(job_path, os.path.join(OUTBOX_DIR, "failed"))
            log_line(f"Job INVALID -> moved to failed: {os.path.basename(moved)}")
            processed += 1
            continue
        except Exception as e:
            err_res = make_result(None, "error", f"unexpected_parse_error: {repr(e)}", {"file": os.path.basename(job_path)})
            out_name = os.path.splitext(os.path.basename(job_path))[0] + ".error.json"
            safe_write_json(os.path.join(RESULTS_DIR, out_name), err_res)
            moved = move_job(job_path, os.path.join(OUTBOX_DIR, "failed"))
            log_line(f"Job PARSE ERROR -> moved to failed: {os.path.basename(moved)}")
            processed += 1
            continue

        # Execute (A2 stub)
        try:
            status, extra = execute_job(job)
            res = make_result(job, status, "job_processed", extra)
            out_name = f"{job.id}.result.json"
            safe_write_json(os.path.join(RESULTS_DIR, out_name), res)

            moved = move_job(job_path, os.path.join(OUTBOX_DIR, "done"))
            log_line(f"Job OK -> result written, moved to done: {os.path.basename(moved)}")
        except Exception as e:
            res = make_result(job, "error", f"execution_error: {repr(e)}")
            out_name = f"{job.id}.error.json"
            safe_write_json(os.path.join(RESULTS_DIR, out_name), res)
            moved = move_job(job_path, os.path.join(OUTBOX_DIR, "failed"))
            log_line(f"Job EXEC ERROR -> moved to failed: {os.path.basename(moved)}")

        processed += 1

    log_line(f"Run complete. processed={processed}")


if __name__ == "__main__":
    main()
